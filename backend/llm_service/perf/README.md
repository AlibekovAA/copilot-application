# Нагрузочное тестирование LLM Service

## Важно

⚠️ **Мок-режим включен автоматически** - все запросы к Mistral AI имитируются без реальных вызовов к API. Это позволяет тестировать производительность Python сервиса без нагрузки на LLM API.

## Установка зависимостей

Используйте Makefile из корня llm_service:

```bash
cd ../..
make install
```

Это установит все зависимости проекта, включая необходимые для нагрузочного тестирования.

Альтернативно, можно установить только необходимую библиотеку:

```bash
pip install aiohttp>=3.13.2
```

## Запуск LLM сервиса с мок-режимом

```bash
docker-compose -f docker-compose.llm-only.yml up --build -d
```

Этот docker-compose автоматически поднимает:

- PostgreSQL базу данных
- Go бекенд (для получения JWT токенов)
- LLM сервис с мок-режимом

Мок-режим включается автоматически через переменную окружения `MOCK_MISTRAL=true`.

## Запуск нагрузочного тестирования

### Базовый запуск (автоматическое получение JWT токена):

```bash
python load_test.py
```

Результаты автоматически сохраняются в файл `results.json` (или в файл, указанный через `--output`).

### С параметрами:

```bash
python load_test.py \
  --url http://localhost:8000 \
  --go-backend-url http://localhost:8080 \
  --users 50 \
  --duration 120 \
  --ramp-up 10 \
  --output my_results.json
```

### Параметры:

- `--url` - Базовый URL LLM сервиса (по умолчанию: http://localhost:8000)
- `--go-backend-url` - URL Go бекенда для получения JWT токена (по умолчанию: http://localhost:8080)
- `--jwt-token` - JWT токен для аутентификации (опционально, если не указан - будет получен автоматически)
- `--users` - Количество concurrent пользователей (по умолчанию: 10)
- `--duration` - Длительность теста в секундах (по умолчанию: 60)
- `--ramp-up` - Время постепенного увеличения нагрузки в секундах (по умолчанию: 0)
- `--think-time` - Пауза между циклами пользователя в секундах (по умолчанию: 0.1)
- `--output` - Имя файла для сохранения результатов в JSON формате (по умолчанию: results.json)

**Важно:** Все результаты сохраняются только в JSON файл. Консольный вывод отсутствует.

### Примеры:

Тест с 100 пользователями на 2 минуты (результаты в `results.json`):

```bash
python load_test.py --users 100 --duration 120
```

Тест с постепенным увеличением нагрузки:

```bash
python load_test.py --users 50 --duration 180 --ramp-up 30
```

Максимальная нагрузка с указанием имени файла:

```bash
python load_test.py \
  --users 200 \
  --duration 300 \
  --think-time 0 \
  --ramp-up 30 \
  --output max_load_results.json
```

Использование своего JWT токена (если Go бекенд недоступен):

```bash
python load_test.py --jwt-token YOUR_TOKEN --users 50 --duration 120 --output custom_results.json
```

## Формат результатов

Все результаты сохраняются в JSON файл. Структура файла включает все метрики тестирования.

## Собираемые метрики

Скрипт собирает следующие метрики:

- **Общие метрики:**

  - Длительность теста
  - Общее количество запросов
  - Количество успешных/неудачных запросов
  - Процент успеха
  - Requests Per Second (RPS)

- **Время отклика:**

  - Min, Max, Mean, Median
  - Процентили: P50, P75, P90, P95, P99
  - Стандартное отклонение

- **Коды ответов:**

  - Распределение HTTP статус кодов
  - Процентное соотношение

- **Ошибки:**

  - Типы ошибок и их количество

- **Метрики по эндпоинтам:**
  - Количество запросов на эндпоинт
  - Среднее время отклика
  - P95 и P99 процентили
  - RPS по эндпоинтам

## Тестируемые эндпоинты

- `POST /conversations` - Создание диалога
- `POST /chat` - Отправка сообщения в чат (с мок-ответом от LLM)
- `GET /conversations` - Получение списка диалогов
- `GET /conversations/{id}/messages` - Получение сообщений диалога
- `GET /health` - Проверка работоспособности

## Мок-режим

При установке переменной окружения `MOCK_MISTRAL=true`:

- Все вызовы к Mistral AI API имитируются
- Возвращается фиктивный ответ с небольшой задержкой (100ms)
- Это позволяет тестировать производительность сервиса без реальных вызовов к LLM API
- База данных работает в обычном режиме (сохраняются все сообщения)

## Остановка сервера

```bash
docker-compose -f docker-compose.llm-only.yml down
```

Для удаления данных БД:

```bash
docker-compose -f docker-compose.llm-only.yml down -v
```
